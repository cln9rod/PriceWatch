# ğŸ§  PriceWatch â€“ System Design (Part 1: Overview)

## ğŸ¯ Goal
Build a lightweight, scalable web app that:
- Allows users to add competitor product URLs
- Periodically scrapes and stores price data
- Alerts users when prices change
- Displays price history in a simple dashboard

## ğŸ—ºï¸ System Overview
**Architecture Style:** Modular monolith (MVP) â†’ Microservices (scale)

### ğŸ§© Core Components
1. **Frontend (UI)** â€“ Streamlit for initial MVP
2. **Scraper** â€“ Playwright-powered headless browser scraper
3. **Database** â€“ SQLite (MVP) â†’ PostgreSQL (scalable)
4. **Scheduler** â€“ APScheduler to run periodic scraping
5. **Notifier** â€“ Email/Telegram alerts
6. **(Optional) API Layer** â€“ FastAPI if REST endpoints are needed later

### ğŸ—ï¸ High-Level Diagram (Text Representation)
```
   +-------------+     +-------------+     +-------------+
   |   Streamlit|<--->|   Database  |<--->|   Scraper    |
   |   Dashboard|     |   SQLite    |     | Playwright  |
   +-------------+     +-------------+     +-------------+
         |                    ^                   ^
         v                    |                   |
+------------------+    +--------------+     +----------+
| Scheduler (cron) |--->| Save to DB   |<----| Notifier |
+------------------+    +--------------+     +----------+
```

---

# ğŸ“¦ Part 2: Components in Detail

## 1. ğŸ›ï¸ Frontend â€“ Streamlit
**Role:** User interface for adding URLs and viewing price history

### Responsibilities:
- Input form to add a new product URL
- Button to trigger manual scrape
- Table or chart to view historical prices
- Flash messages for latest price

### Benefits:
- Fast development
- Lightweight & interactive

### Drawbacks:
- Not suited for multi-user auth or real-time data

â¡ï¸ Future Upgrade: React + FastAPI UI

---

## 2. ğŸ•·ï¸ Scraper â€“ Playwright
**Role:** Extract current price from a product page

### Responsibilities:
- Load product page in a headless browser
- Use a CSS selector to extract the price
- Handle JavaScript rendering (e.g., SPAs like Amazon)

### Benefits:
- Reliable and compatible with most modern sites

### Drawbacks:
- Resource-intensive
- Slower than simple requests/BeautifulSoup

â¡ï¸ Future Upgrade: Rotate proxies / headless containers for scale

---

# ğŸ—ƒï¸ Part 3: Database â€“ SQLite (MVP)

## ğŸ§± Purpose:
Store product URLs, scraped prices, and timestamps.

### Tables:
- **products** (optional, for future expansion):
  - id
  - url
  - label

- **prices**:
  - id
  - url
  - price
  - timestamp

### Benefits:
- Easy setup, no server needed
- Great for MVP/local usage

### Drawbacks:
- Not optimized for heavy concurrent writes
- Not cloud-native

â¡ï¸ Future Upgrade: PostgreSQL + SQLAlchemy + Cloud hosting

---

# â° Part 4: Scheduler â€“ APScheduler

## ğŸ” Purpose:
Trigger scraping jobs automatically on a time interval.

### How It Works:
- Load list of tracked URLs from the DB
- Run scraper for each one
- Save results back to DB
- (Optional) Trigger notification if price changed

### Benefits:
- Python-native, simple to implement
- Can run as a separate service or integrated process

### Drawbacks:
- Needs proper logging & error handling

â¡ï¸ Future Upgrade: Celery + Redis for distributed task queues

---

# ğŸ”” Part 5: Notifier â€“ Alerts via Email/Telegram

## ğŸ“¢ Purpose:
Alert users when price changes significantly.

### Channels:
- **Email (SMTP)**
  - Use Gmail, Mailgun, or SMTP server
- **Telegram Bot API**
  - Setup bot â†’ send message to chat ID

### Alert Logic:
- Compare new price with last stored price
- If delta > threshold â†’ send alert

### Benefits:
- Immediate feedback for users
- Low cost to operate

â¡ï¸ Future Upgrade: Push notifications, Slack, Webhooks

---

# ğŸŒ Part 6: (Optional) API & Deployment

## FastAPI (Optional API Layer)
**Use case:** Expose REST API to allow external apps or frontend clients to:
- Add/remove URLs
- Trigger scrape manually
- Retrieve price history as JSON

## Deployment Options
- **MVP**: Streamlit Cloud (free hosting)
- **Backend**: Render, Railway, Fly.io for cron/scheduler & scraper
- **Database**: SQLite (local), PostgreSQL on Supabase or Neon
- **Custom domain**: Vercel + Framer for landing page

---

âœ… This breakdown keeps each part modular, beginner-friendly, and ready to scale gradually.
